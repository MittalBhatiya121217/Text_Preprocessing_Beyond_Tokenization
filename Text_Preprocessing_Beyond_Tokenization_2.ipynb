{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-K9BsX3KcC6y"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8hFNQohPORn"
      },
      "source": [
        "# Text Preprocessing Beyond Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peiyYN47rMy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c05b6a-139e-449e-84ec-76fac48d1bd1"
      },
      "source": [
        "import re\n",
        "# for using NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# for using SpaCy\n",
        "import spacy\n",
        "\n",
        "# for HuggingFace\n",
        "!pip install transformers\n",
        "# !pip install ftfy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we try to install libraries.\n",
        "re - is the regular expression libraries is used for searching for matching patterns from the string.\n",
        "\n",
        "nltk - nltk stands for natural language processing toolkit its use for processing text data and its come with lots of test dataset.\n",
        "\n",
        "nltk_download('punkt') -here we try to download nltk for punktuation. punkt is used for divided text into list of senteces and for that its use supervised algorithm for abbreviation words, collation, and start with sentences with the words.\n",
        "\n",
        "nltk_download('stopwords') - here, we try to download the stopwords.stopwords are a, an, the, is, etc. when we try to search the pattern that stopwords doesn't use for search pattern matching.\n",
        "\n",
        "nltk_download('wordnet') - here we download wordnet. wordnet is english dataset of adjectives, verbs, noun and adverbs.\n",
        "\n",
        "nltk.corpus - corpus is used for corpus reader class.\n",
        "\n",
        "import spacy - spacy is used for information extraction or build natural language understanding system.\n",
        "\n",
        "!pip install transformers - transformers used to clean, reduce, expand or generate features."
      ],
      "metadata": {
        "id": "SAiW2P7jOJrI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3Bh5T8sJXFQ"
      },
      "source": [
        "# trick to wrap text to the viewing window for this notebook\n",
        "# Ref: https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ipython - ipython is used for import display for html code.\n",
        "\n",
        "def set_css() - this is used for set up the style sheet of css(casecading style sheet)"
      ],
      "metadata": {
        "id": "EwBF4tLFcjJL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxtJzrzGXchm"
      },
      "source": [
        "## **(Tutorial) Tokenizing text using Spacy**\n",
        "\n",
        "Following is a sample of text to demonstrate tokenization in SpaCy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP9CaD6ufac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "9c261c87-1831-447c-d40d-449b0757e31b"
      },
      "source": [
        "dummy_text1 = \"\"\"Here is the First Paragraph and this is the First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the first paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
        "Now, it is the Second Paragraph and its First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the second paragraph. This paragraph is ending now with a Fifth Sentence.\n",
        "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the third paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
        "4th paragraph just has one sentence in it.\n",
        "\"\"\"\n",
        "\n",
        "print(dummy_text1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the First Paragraph and this is the First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the first paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
            "Now, it is the Second Paragraph and its First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the second paragraph. This paragraph is ending now with a Fifth Sentence.\n",
            "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the third paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
            "4th paragraph just has one sentence in it.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is just simple dummy text printed."
      ],
      "metadata": {
        "id": "NNdA0ZRSfhkP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6SZCwFAfxfI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2672
        },
        "outputId": "0935ed45-6dd1-47bf-ed26-027f4499f29a"
      },
      "source": [
        "# loads a trained English pipeline with specific preprocessing components\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# using SpaCy's tokenizer...\n",
        "doc = nlp(dummy_text1)      # applies the processing pipeline on the text\n",
        "for token in doc:\n",
        "  print(token.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here\n",
            "is\n",
            "the\n",
            "First\n",
            "Paragraph\n",
            "and\n",
            "this\n",
            "is\n",
            "the\n",
            "First\n",
            "Sentence\n",
            ".\n",
            "Here\n",
            "is\n",
            "the\n",
            "Second\n",
            "Sentence\n",
            ".\n",
            "Now\n",
            "is\n",
            "the\n",
            "Third\n",
            "Sentence\n",
            ".\n",
            "This\n",
            "is\n",
            "the\n",
            "Fourth\n",
            "Sentence\n",
            "of\n",
            "the\n",
            "first\n",
            "paragaraph\n",
            ".\n",
            "This\n",
            "paragraph\n",
            "is\n",
            "ending\n",
            "now\n",
            "with\n",
            "a\n",
            "Fifth\n",
            "Sentence\n",
            ".\n",
            "\n",
            "\n",
            "Now\n",
            ",\n",
            "it\n",
            "is\n",
            "the\n",
            "Second\n",
            "Paragraph\n",
            "and\n",
            "its\n",
            "First\n",
            "Sentence\n",
            ".\n",
            "Here\n",
            "is\n",
            "the\n",
            "Second\n",
            "Sentence\n",
            ".\n",
            "Now\n",
            "is\n",
            "the\n",
            "Third\n",
            "Sentence\n",
            ".\n",
            "This\n",
            "is\n",
            "the\n",
            "Fourth\n",
            "Sentence\n",
            "of\n",
            "the\n",
            "second\n",
            "paragraph\n",
            ".\n",
            "This\n",
            "paragraph\n",
            "is\n",
            "ending\n",
            "now\n",
            "with\n",
            "a\n",
            "Fifth\n",
            "Sentence\n",
            ".\n",
            "\n",
            "\n",
            "Finally\n",
            ",\n",
            "this\n",
            "is\n",
            "the\n",
            "Third\n",
            "Paragraph\n",
            "and\n",
            "is\n",
            "the\n",
            "First\n",
            "Sentence\n",
            "of\n",
            "this\n",
            "paragraph\n",
            ".\n",
            "Here\n",
            "is\n",
            "the\n",
            "Second\n",
            "Sentence\n",
            ".\n",
            "Now\n",
            "is\n",
            "the\n",
            "Third\n",
            "Sentence\n",
            ".\n",
            "This\n",
            "is\n",
            "the\n",
            "Fourth\n",
            "Sentence\n",
            "of\n",
            "the\n",
            "third\n",
            "paragaraph\n",
            ".\n",
            "This\n",
            "paragraph\n",
            "is\n",
            "ending\n",
            "now\n",
            "with\n",
            "a\n",
            "Fifth\n",
            "Sentence\n",
            ".\n",
            "\n",
            "\n",
            "4th\n",
            "paragraph\n",
            "just\n",
            "has\n",
            "one\n",
            "sentence\n",
            "in\n",
            "it\n",
            ".\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spacy.load() - is used for wrapping. its read the pipeline config. cfg, use for construct the model object and load the data model."
      ],
      "metadata": {
        "id": "M3PFaP7MgIDH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWJsFBrePj_o"
      },
      "source": [
        "### **Task 1. Revisiting Tokenization**\n",
        "\n",
        "Let's start with using the WordPunctTokenizer using NLTK.\n",
        "\n",
        "WordPunctTokenizer seperates each character in the text and shows them as a seperate unit. This method helps us to get the tokens from sentences as alphabatic and non-alphabetic characters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci59Ry_hQq_C"
      },
      "source": [
        "#### **Question 1a. Implement the WordPunctTokenizer method for the given text in the code block below and write down the observations in your own words .(5 points)**\n",
        "\n",
        "**Important Note:**\n",
        "1. DO NOT use any of the existing implementations for tokenization distributed as part of open-source NLP libraries.\n",
        "2. **If your solution uses readily available implementations of tokenizers, you will receive zero credit for this question.**\n",
        "3. Your tokenizer implentation need not be the most optimized one. It should just be able to get the job done. You can also ignore punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQI_o_IEfR5j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "daeb772e-ef8e-4c30-e54d-6293cd0c1f68"
      },
      "source": [
        "sample_text=\"\"\"Lorem ipsum is a substitute text that is frequently used in publication and graphic arts to display the visual shape of a paper or a typeface without depending on significant content.\n",
        "Before the final version is available, lorem ipsum can be used as a stand-in. A technique known as greeking, which enables designers to think about the shape of a website or publishing without the significance of the text affecting the layout,\n",
        "also uses it to try and replace text.The Latin phrase lorem ipsum is usually a mutated version of the first-century BC treatise De finibus bonorum et malorum by the Roman politician and scholar Marcus.Tullius.Cicero,\n",
        "with phrases changed, added, and deleted to make it sound absurd and indecent.\"\"\"\n",
        "\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "# add your code below this comment and execute it once you have written the code\n",
        "txt = WordPunctTokenizer()\n",
        "text = txt.tokenize(sample_text)\n",
        "print(text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem', 'ipsum', 'is', 'a', 'substitute', 'text', 'that', 'is', 'frequently', 'used', 'in', 'publication', 'and', 'graphic', 'arts', 'to', 'display', 'the', 'visual', 'shape', 'of', 'a', 'paper', 'or', 'a', 'typeface', 'without', 'depending', 'on', 'significant', 'content', '.', 'Before', 'the', 'final', 'version', 'is', 'available', ',', 'lorem', 'ipsum', 'can', 'be', 'used', 'as', 'a', 'stand', '-', 'in', '.', 'A', 'technique', 'known', 'as', 'greeking', ',', 'which', 'enables', 'designers', 'to', 'think', 'about', 'the', 'shape', 'of', 'a', 'website', 'or', 'publishing', 'without', 'the', 'significance', 'of', 'the', 'text', 'affecting', 'the', 'layout', ',', 'also', 'uses', 'it', 'to', 'try', 'and', 'replace', 'text', '.', 'The', 'Latin', 'phrase', 'lorem', 'ipsum', 'is', 'usually', 'a', 'mutated', 'version', 'of', 'the', 'first', '-', 'century', 'BC', 'treatise', 'De', 'finibus', 'bonorum', 'et', 'malorum', 'by', 'the', 'Roman', 'politician', 'and', 'scholar', 'Marcus', '.', 'Tullius', '.', 'Cicero', ',', 'with', 'phrases', 'changed', ',', 'added', ',', 'and', 'deleted', 'to', 'make', 'it', 'sound', 'absurd', 'and', 'indecent', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "wordpuncttokenizer() - Here, we use wordpunctTokenizer() for tokenize sentences into token as a alphabetic and non-alphabetic character by words.\n",
        "\n",
        "output - in this output we can see that wordpuncttokenizer() is tokenize whole the sentence in to token and add periods (\".\") and comma(\",\"). here based on the results its shows that its tokenize with '-' , '.', ','. here stand-in is whole word but here its split the word with '-' and devided in to three tokens.\n",
        "another word first-century is also split into three differnet tokens \"first\",\"-\",\"century\". so its only devide the whole paragraph into tokens and its part tokens of each words and punctuations also taken as a tokens."
      ],
      "metadata": {
        "id": "52JnANebl3Bq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC3EqwmTSswZ"
      },
      "source": [
        "####**Question 1b.** Implement the tokenizers given below. Analyze how the words are being tokenized for both the tokenizers. Differentiate the outputs of the tokenizers and write them down in your own words.(10 points)\n",
        "1. **NLTK's Word tokenizer**\n",
        "2. **NLTK's Punctuation-based tokenizer**\n",
        "\n",
        "**Note:** You are already familiar with using NLTK's tokenization which was demosntrated in the previous labs. If you do not remember, just revisit them to refresh your memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8nGNxYKQqeJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "806a8605-5657-4171-ef94-76524cf2d18b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "sample_text=\"\"\"Lorem ipsum is a substitute text that is frequently used in publication and graphic arts to display the visual shape of a paper or a typeface without depending on significant content.\n",
        "Before the final version is available, lorem ipsum can be used as a stand-in. A technique known as greeking, which enables designers to think about the shape of a website or publishing without the significance of the text affecting the layout,\n",
        "also uses it to try and replace text.The Latin phrase lorem ipsum is usually a mutated version of the first-century BC treatise De finibus bonorum et malorum by the Roman politician and scholar Marcus.Tullius.Cicero,\n",
        "with phrases changed, added, and deleted to make it sound absurd and indecent.\"\"\"\n",
        "\n",
        "from nltk.tokenize import(word_tokenize, wordpunct_tokenize)\n",
        "\n",
        "# add your code below this comment and execute it once you have written the code.\n",
        "# you can additional code cells if need be. make sure to use the text cell provided to answer the question.\n",
        "#code for NLTK's word Tokenizer\n",
        "tokens = word_tokenize(sample_text)\n",
        "print(word_tokenize(sample_text))\n",
        "print(\" \")\n",
        "#Code for NLTK's Punctuation-based tokenizer\n",
        "txt = WordPunctTokenizer()\n",
        "text = txt.tokenize(sample_text)\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem', 'ipsum', 'is', 'a', 'substitute', 'text', 'that', 'is', 'frequently', 'used', 'in', 'publication', 'and', 'graphic', 'arts', 'to', 'display', 'the', 'visual', 'shape', 'of', 'a', 'paper', 'or', 'a', 'typeface', 'without', 'depending', 'on', 'significant', 'content', '.', 'Before', 'the', 'final', 'version', 'is', 'available', ',', 'lorem', 'ipsum', 'can', 'be', 'used', 'as', 'a', 'stand-in', '.', 'A', 'technique', 'known', 'as', 'greeking', ',', 'which', 'enables', 'designers', 'to', 'think', 'about', 'the', 'shape', 'of', 'a', 'website', 'or', 'publishing', 'without', 'the', 'significance', 'of', 'the', 'text', 'affecting', 'the', 'layout', ',', 'also', 'uses', 'it', 'to', 'try', 'and', 'replace', 'text.The', 'Latin', 'phrase', 'lorem', 'ipsum', 'is', 'usually', 'a', 'mutated', 'version', 'of', 'the', 'first-century', 'BC', 'treatise', 'De', 'finibus', 'bonorum', 'et', 'malorum', 'by', 'the', 'Roman', 'politician', 'and', 'scholar', 'Marcus.Tullius.Cicero', ',', 'with', 'phrases', 'changed', ',', 'added', ',', 'and', 'deleted', 'to', 'make', 'it', 'sound', 'absurd', 'and', 'indecent', '.']\n",
            " \n",
            "['Lorem', 'ipsum', 'is', 'a', 'substitute', 'text', 'that', 'is', 'frequently', 'used', 'in', 'publication', 'and', 'graphic', 'arts', 'to', 'display', 'the', 'visual', 'shape', 'of', 'a', 'paper', 'or', 'a', 'typeface', 'without', 'depending', 'on', 'significant', 'content', '.', 'Before', 'the', 'final', 'version', 'is', 'available', ',', 'lorem', 'ipsum', 'can', 'be', 'used', 'as', 'a', 'stand', '-', 'in', '.', 'A', 'technique', 'known', 'as', 'greeking', ',', 'which', 'enables', 'designers', 'to', 'think', 'about', 'the', 'shape', 'of', 'a', 'website', 'or', 'publishing', 'without', 'the', 'significance', 'of', 'the', 'text', 'affecting', 'the', 'layout', ',', 'also', 'uses', 'it', 'to', 'try', 'and', 'replace', 'text', '.', 'The', 'Latin', 'phrase', 'lorem', 'ipsum', 'is', 'usually', 'a', 'mutated', 'version', 'of', 'the', 'first', '-', 'century', 'BC', 'treatise', 'De', 'finibus', 'bonorum', 'et', 'malorum', 'by', 'the', 'Roman', 'politician', 'and', 'scholar', 'Marcus', '.', 'Tullius', '.', 'Cicero', ',', 'with', 'phrases', 'changed', ',', 'added', ',', 'and', 'deleted', 'to', 'make', 'it', 'sound', 'absurd', 'and', 'indecent', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer for Q1b.** Type in your answer here!\n",
        "\n",
        "Here, we use two different model for tokenize sentences in to tokens.\n",
        "\n",
        "word_tokenize() - this is used for split sentences into words using natural language processing(NLTK) libraries toolkit.\n",
        "\n",
        "WordPunctTokenizer() - this is used for split tokens from the words and sentences. its contain alphabetic and non-alphabetic characters."
      ],
      "metadata": {
        "id": "8VvQYmDk-w8l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgaX-Ck7YYzY"
      },
      "source": [
        "**Different between word_tokenize() vs WordPunctTokenizer()**\n",
        "\n",
        "The main difference between two method is word_tokenize() method take 'stand-in' as a whole word however, WordPunctTokenizer() method is take 'stand-in' as three different tokens 'stand' ,'-', 'in'.\n",
        "\n",
        "In this paragraph we see that the words 'Marcus.Tullius.Cicero' take as a  single word while using word_tokenize() method while using WordPunctTokenizer() method devides into 5 tokens as 'Marcus','.','Tullius','.','Cicero'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNUmlAHIQ9gs"
      },
      "source": [
        "## **(Tutorial) Stemming and Lemmatization using NLTK**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute and understand the following code in order to get the basic understanding of how the porter stemmer algorithm works."
      ],
      "metadata": {
        "id": "5w2DnTuhGaSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing PorterStemmer class from nltk.stem module\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()    # instantiating an object of the PorterStemmer class\n",
        "\n",
        "stem = porter.stem('cats')    # calling the stemmer algorithm on the desired word\n",
        "print(f\"'cats' after stemming: {stem}\")"
      ],
      "metadata": {
        "id": "t5gZ04EnGYy8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9f4bc62-4032-4942-fe31-10a0beb80639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'cats' after stemming: cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Porter Stemming algorithm:**\n",
        "Porter Stemming algorithm was presented by Porter,M. this algorithm is used for stemming. stemming is the process of converting words in to the root or base word. this algorithm reduced the word from 'having' to 'have'.\n",
        "\n",
        "we first get the library nltk.stem and import porterstemmer. then initialize the object to the porter stemmer. now we call the stem() method with name of the word which is we want to stem and then print the stemming word with its base form."
      ],
      "metadata": {
        "id": "yETjTCXpIyeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try executing the porter stemmer on your own examples**"
      ],
      "metadata": {
        "id": "FV2d70m4KjE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter your code here\n",
        "# importing PorterStemmer class from nltk.stem module\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()    # instantiating an object of the PorterStemmer class\n",
        "\n",
        "stem = porter.stem('having')    # calling the stemmer algorithm on the desired word\n",
        "print(f\"'having' after stemming: {stem}\")"
      ],
      "metadata": {
        "id": "TvgQA-zRKoeS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d59c2e95-97e8-4160-9b16-937039214689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'having' after stemming: have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we try to use the same algorithm with the different word. stemming is basically use to remove 'ing','ed','ss','s','es' its transform the words into the basic root form. here i am using  ing  word 'having' remove ing and convert into the base form 'have'."
      ],
      "metadata": {
        "id": "9k-5fUoRQ5wf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jCebsYwiQxf"
      },
      "source": [
        "**Let's see how we can perform stemming and lemmatization using NLTK library.**\n",
        "\n",
        "The Lancaster stemmer algorithm is a series of rules that specifies the substitution or removal of an ending. Unlike the porter stemmer algorithm it applies heavy stemming on the text. For instance, using LancasterStemmer, destabilized will be stemmed to dest and porter stemmer will stem it to destabl. Due to iterations and over-stemming, LancasterStemmer creates a stem that is even smaller than Porter's.\n",
        "\n",
        "Below is the implementation of Lancaster stemming algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3M0mDUIiSdu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bcf5f697-bb57-4127-9bad-22e63856b88b"
      },
      "source": [
        "# importing LancasterStemmer class from nltk.stem module\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "lancaster=LancasterStemmer()    # instantiating an object of the LancasterStemmer class\n",
        "\n",
        "for S in [\"cats\"]: # Used a list as you will be working with a list in the below questions\n",
        "  print(lancaster.stem(S))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we try to use lancaster stemmer algorithm which is used as extremly aggresive algorithm. this algorithm is provide the functionality to do customize the rules of algorithms.\n",
        "\n",
        "we first use nltk.stem and import LancasterStemmer algorithm and nltk.tokenize and import sent_tokenize, word_tokenize.\n",
        "\n",
        "nltk.stem - natural language processing toolkit with stem is used as a package that perform stemming process.\n",
        "\n",
        "LancasterStemmer - LancasterStemmer is the algorithm for stemming.\n",
        "\n",
        "nltk.tokenize - nltk is the library used for pattern search and making tokens from the sentences.\n",
        "\n",
        "sent_tokenize - this is used for split string into multiple sentences.\n",
        "\n",
        "word_tokenize - this is used for split string into words."
      ],
      "metadata": {
        "id": "mFw6gVwzTa4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try executing the Lancaster stemmer on your own examples (2 points)**"
      ],
      "metadata": {
        "id": "H_p-k7WyAj59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter your code here\n",
        "# importing LancasterStemmer class from nltk.stem module\n",
        "words = ['regarding','ranges','kindly','nonobjectivity']\n",
        "\n",
        "Lanc = LancasterStemmer()\n",
        "\n",
        "for wrd in words:\n",
        "\n",
        "    print(wrd, \" : \", Lanc.stem(wrd))"
      ],
      "metadata": {
        "id": "Mjmkh8h5--gU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "7a214482-2e86-400d-981f-c25dc105a1ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "regarding  :  regard\n",
            "ranges  :  rang\n",
            "kindly  :  kind\n",
            "nonobjectivity  :  nonobject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we try to use the LancasterStemmer algorithm for stemming words. here i defined 4 different words and after that make an object for lancasterstemmer() after that use for loop for printing words after using lancasterstemmer().\n",
        "\n",
        "output : Here, we use lancasterstemmer() for word stemming. its shows the regarding -> regrad, ranges -> rang, kindly -> kind, nonobjectivity -> nonobject. lancaster remove the ending and print them."
      ],
      "metadata": {
        "id": "Rz8Ll1aNwJHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will see the implementation of Lemmatization using textblob method.\n",
        "\n",
        "Python's TextBlob package is used to process textual data. It offers a straightforward API for getting started with typical natural language processing (NLP) activities like part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and others.\n",
        "\n",
        "**Installing some libraries for performing Lemmatization using textblob method**"
      ],
      "metadata": {
        "id": "6C1jPHmF12jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "!pip install textblob\n",
        "!python -m textblob.download_corpora\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "uAKhV2D116_U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "d1bca7ed-d885-4cb1-fa46-7b4f27bccf0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.8/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.8/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we try to install nltk libraries.Install textblob and other libraries.\n",
        "\n",
        "nltk - natural language toolkit for  processing text.\n",
        "\n",
        "textblob - textblob is used for processing text based data. its use for sentiment analysis, part of speech tagging etc.\n",
        "\n",
        "textblob.download_corpora - this used for getting some basic functionality.\n",
        "\n",
        "nltk.download('omw-1.4') - nltk download omw. omw stands for Open Multilingual Wordnet. its just like wordnet but using this insted of using omw.\n",
        "\n",
        "lemmatization - lemmatization is used for coverting words into the base form."
      ],
      "metadata": {
        "id": "9taw8os22sXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob, Word\n",
        "\n",
        "my_word = 'cars'\n",
        "\n",
        "# create a Word object\n",
        "w = Word(my_word)\n",
        "\n",
        "print(w.lemmatize())"
      ],
      "metadata": {
        "id": "OVEs7IyB2BSf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c1bb57a-02aa-422e-eee4-c98813f8fad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we try to download textblob for process text based data with word. then set up one word 'cars' then we create one object for that word  and at the last print that word with lemmatize() method.\n",
        "\n",
        "lemmatize() - lemmatize used for covert different forms of words into the root based words."
      ],
      "metadata": {
        "id": "ElmGpaBD7EFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execute lemmatization with textblob on your own examples (3 points)**"
      ],
      "metadata": {
        "id": "pZg6RcS0A4y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter your code here\n",
        "from textblob import Word\n",
        "\n",
        "# create object.\n",
        "u = Word(\"bricks\")\n",
        "\n",
        "# apply lemmatization to the word bricks.\n",
        "print(\"bricks :\", u.lemmatize())\n",
        "\n",
        "# create object.\n",
        "v = Word(\"ultra\")\n",
        "\n",
        "# apply lemmatization to the word ultra.\n",
        "print(\"ultra :\", v.lemmatize())\n",
        "\n",
        "# create object.\n",
        "w = Word(\"joker\")\n",
        "\n",
        "# apply lemmatization with to the word joker.\n",
        "print(\"joker :\", w.lemmatize())"
      ],
      "metadata": {
        "id": "0m8icXUx_SUc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "8e3a4a93-6535-4d4a-dbe3-c94936a0ab14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bricks : brick\n",
            "ultra : ultra\n",
            "joker : joker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we try to use textblob for different word processing.\n",
        "\n",
        "first we import word library from textblob.\n",
        "we create object for words.\n",
        "we print this word with using lemmatize().\n",
        "\n",
        "outpur : here we use lemmatize() method with each words. bricks -> brick, ultra -> ultra, joker -> joker. In conclusion, lemmatize convert different form of words into the base form."
      ],
      "metadata": {
        "id": "PiwuTRjd-wn5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDGVeoHqrQkF"
      },
      "source": [
        "### **Task 2: Lemmatization or Stemming?**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3K_-XgVmKin"
      },
      "source": [
        "Following is the text that you will be using for this task (Task 2 only):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCtK0QouYj2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "47b0dc4d-17ec-42a6-dcfb-178749627d38"
      },
      "source": [
        "# This is the text on which you have to perform stemming; taken from Internet.\n",
        "text = \"In grammar, inflection is the modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood. An inflection expresses one or more grammatical categories with a prefix, suffix or infix, or another internal modification such as a vowel change. Stem (root) is the part of the word to which you add inflectional (changing/deriving) affixes such as (-ed,-ize, -s,-de,mis)\"\n",
        "#print(\"Given text:\")\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In grammar, inflection is the modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood. An inflection expresses one or more grammatical categories with a prefix, suffix or infix, or another internal modification such as a vowel change. Stem (root) is the part of the word to which you add inflectional (changing/deriving) affixes such as (-ed,-ize, -s,-de,mis)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example of Lemmatization.**"
      ],
      "metadata": {
        "id": "z39t_mjSIqPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "# Tokenize: Split the sentence into words using word_tokenize()\n",
        "words = nltk.word_tokenize(text)\n",
        "print(words)\n",
        "print(' ')\n",
        "# Lemmatize list of the words and join that words.\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = nltk.word_tokenize(text)\n",
        "lemmatize_result = ' '.join([lemmatizer.lemmatize(wrd) for wrd in words])\n",
        "print(lemmatize_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "h0xiOOEoIqp2",
        "outputId": "263d902d-dc02-4a3b-ef58-10e58053b07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'grammar', ',', 'inflection', 'is', 'the', 'modification', 'of', 'a', 'word', 'to', 'express', 'different', 'grammatical', 'categories', 'such', 'as', 'tense', ',', 'case', ',', 'voice', ',', 'aspect', ',', 'person', ',', 'number', ',', 'gender', ',', 'and', 'mood', '.', 'An', 'inflection', 'expresses', 'one', 'or', 'more', 'grammatical', 'categories', 'with', 'a', 'prefix', ',', 'suffix', 'or', 'infix', ',', 'or', 'another', 'internal', 'modification', 'such', 'as', 'a', 'vowel', 'change', '.', 'Stem', '(', 'root', ')', 'is', 'the', 'part', 'of', 'the', 'word', 'to', 'which', 'you', 'add', 'inflectional', '(', 'changing/deriving', ')', 'affixes', 'such', 'as', '(', '-ed', ',', '-ize', ',', '-s', ',', '-de', ',', 'mis', ')']\n",
            " \n",
            "In grammar , inflection is the modification of a word to express different grammatical category such a tense , case , voice , aspect , person , number , gender , and mood . An inflection express one or more grammatical category with a prefix , suffix or infix , or another internal modification such a a vowel change . Stem ( root ) is the part of the word to which you add inflectional ( changing/deriving ) affix such a ( -ed , -ize , -s , -de , mi )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we use lemmatization for text process. here we use nltk.stem download libraries and import wordnetlemmatizer.\n",
        "\n",
        "we make object for word tokenize for text. now print thoes words. use use normal word_tokenize() method for tokenize.\n",
        "\n",
        "after that we make object for method wordnetlemmatizer(). after that we join thoes words using lemmatize() method for join that words and at the last print the whole sentences."
      ],
      "metadata": {
        "id": "nvSeWnfSLQoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example of Stemming.**"
      ],
      "metadata": {
        "id": "nLNXAiZvIrVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the text on which you have to perform stemming; taken from Internet.\n",
        "text = \"In grammar, inflection is the modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood. An inflection expresses one or more grammatical categories with a prefix, suffix or infix, or another internal modification such as a vowel change. Stem (root) is the part of the word to which you add inflectional (changing/deriving) affixes such as (-ed,-ize, -s,-de,mis)\"\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "lanca_stemmer = LancasterStemmer()\n",
        "nltk_tokens = nltk.word_tokenize(text)\n",
        "stemmer_result = ' '.join([lanca_stemmer.stem(wrd) for wrd in nltk_tokens])\n",
        "print(stemmer_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "PG5UkbfDIrt-",
        "outputId": "eaac3506-130d-41ed-e12d-bc28cecd9495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in gramm , inflect is the mod of a word to express diff gram categ such as tens , cas , voic , aspect , person , numb , gend , and mood . an inflect express on or mor gram categ with a prefix , suffix or infix , or anoth intern mod such as a vowel chang . stem ( root ) is the part of the word to which you ad inflect ( changing/deriv ) affix such as ( -ed , -ize , -s , -de , mis )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we try to use lancasterstemmer for stemming the words.\n",
        "\n",
        "we use nltk.stem.lancaster libraries. we create object for lancasterstemmer() method after that we use word_tokens() and print.\n",
        "\n",
        "now we use for loop for print stemming words. print actual word as well as stem words. word 'grammer' convert to 'gram'."
      ],
      "metadata": {
        "id": "zJTd-WuSdHbw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxubbVwPmSsO"
      },
      "source": [
        "Performing some preprocessing that we have learnt in previous ICEs..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el7w7c7HmY9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "936d0bb0-f4e4-4ce8-c656-038f9a278bde"
      },
      "source": [
        "text = \"In grammar, inflection is the modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood. An inflection expresses one or more grammatical categories with a prefix, suffix or infix, or another internal modification such as a vowel change. Stem (root) is the part of the word to which you add inflectional (changing/deriving) affixes such as (-ed,-ize, -s,-de,mis)\"\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "en_stopwords = stopwords.words('english')\n",
        "\n",
        "\n",
        "#remove punctuation from the text.\n",
        "import re\n",
        "clean_text = re.sub(r\"[^a-zA-Z0-9 ]\", r\" \", text.lower())\n",
        "print(clean_text)\n",
        "print(\" \")\n",
        "#remove stopwords from the text.\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(clean_text)\n",
        "# converts the words in tokens to lowercase and then checks whether they are available in stop_words or not\n",
        "final_text = [w for w in clean_text.split(' ') if not w.lower() in stop_words]\n",
        "final_text = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        final_text.append(w)\n",
        "print(\"After remove stowords\")\n",
        "print(final_text)\n",
        "final_texts = (\" \").join(final_text)\n",
        "\n",
        "#print(word_tokens)\n",
        "print(\" \")\n",
        "print(final_texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in grammar  inflection is the modification of a word to express different grammatical categories such as tense  case  voice  aspect  person  number  gender  and mood  an inflection expresses one or more grammatical categories with a prefix  suffix or infix  or another internal modification such as a vowel change  stem  root  is the part of the word to which you add inflectional  changing deriving  affixes such as   ed  ize   s  de mis \n",
            " \n",
            "After remove stowords\n",
            "['grammar', 'inflection', 'modification', 'word', 'express', 'different', 'grammatical', 'categories', 'tense', 'case', 'voice', 'aspect', 'person', 'number', 'gender', 'mood', 'inflection', 'expresses', 'one', 'grammatical', 'categories', 'prefix', 'suffix', 'infix', 'another', 'internal', 'modification', 'vowel', 'change', 'stem', 'root', 'part', 'word', 'add', 'inflectional', 'changing', 'deriving', 'affixes', 'ed', 'ize', 'de', 'mis']\n",
            " \n",
            "grammar inflection modification word express different grammatical categories tense case voice aspect person number gender mood inflection expresses one grammatical categories prefix suffix infix another internal modification vowel change stem root part word add inflectional changing deriving affixes ed ize de mis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we use searching pattern based on english grammers. and we try to spit data based on white spaces. we remove stopwords of english from the text-string.\n",
        "\n",
        "remove stopwords and split that text string into tokens.here, i used text split and convert text into lower case and check wether its available in stop words.print final text without stop words."
      ],
      "metadata": {
        "id": "AoqUGxygEa6J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac5uju7eGVfg"
      },
      "source": [
        "#### **Question 2. Remove punctuation and stopwords from the text using the functions provided above.Then perform stemming on the cleaned text using the Lancaster Stemmer from NLTK.(10 points)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAC8FFLCErdI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "2856d158-20d5-47ec-ee01-98cfcb8b5afe"
      },
      "source": [
        "# apply Lancaster Stemmer on the cleaned text (after punctuation and stopwords are removed) below this comment\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "print(final_texts)\n",
        "print(\" \")\n",
        "lanca_stemmer = LancasterStemmer()\n",
        "nltk_tokens = nltk.word_tokenize(final_texts)\n",
        "stemmer_result = ' '.join([lanca_stemmer.stem(wrd) for wrd in nltk_tokens])\n",
        "print(stemmer_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grammar inflection modification word express different grammatical categories tense case voice aspect person number gender mood inflection expresses one grammatical categories prefix suffix infix another internal modification vowel change stem root part word add inflectional changing deriving affixes ed ize de mis\n",
            " \n",
            "gramm inflect mod word express diff gram categ tens cas voic aspect person numb gend mood inflect express on gram categ prefix suffix infix anoth intern mod vowel chang stem root part word ad inflect chang der affix ed iz de mis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we used lancasterstemmer with cleaned text after removing stop words and punctuation. we can see that how the lancaster stemmer worked with text.\n",
        "\n",
        "final result : Here, focusing on the final result its shows some of the words converted into base form and some ending part remove by the lancaster algorithm.\n",
        "e.g. inflection convert to inflect\n",
        "     tense taken as a same base word.\n",
        "\n",
        "lancaterstammer() - lancasterstammer is used for heavy iteration algorithm and there is issue of over iteration.\n",
        "\n",
        "stem() - stem() method is used for stemming words.\n"
      ],
      "metadata": {
        "id": "l3kqniks3lfq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N44TNDasS6eK"
      },
      "source": [
        "#### **Question 3. Perform lemmatization on the same cleaned text above using textblob lemmatizer.(10 points)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ-moA25Erh3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "bac733aa-b3d6-4950-9438-023e956a85d3"
      },
      "source": [
        "# apply NLTK's textblob lemmatizer on the cleaned text (after punctuation and stopwords are removed) below this comment\n",
        "from textblob import TextBlob, Word\n",
        "\n",
        "print(final_texts)\n",
        "print(\" \")\n",
        "lem = []\n",
        "for i in final_text:\n",
        "\n",
        "   word1 = Word(i).lemmatize(\"n\")\n",
        "   word2 = Word(word1).lemmatize(\"v\")\n",
        "   word3 = Word(word2).lemmatize(\"a\")\n",
        "   lem.append(Word(word3).lemmatize())\n",
        "   lems = (\" \").join(lem)\n",
        "print(\"Lemmatize using TextBlob\")\n",
        "print(lems)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grammar inflection modification word express different grammatical categories tense case voice aspect person number gender mood inflection expresses one grammatical categories prefix suffix infix another internal modification vowel change stem root part word add inflectional changing deriving affixes ed ize de mis\n",
            " \n",
            "Lemmatize using TextBlob\n",
            "grammar inflection modification word express different grammatical category tense case voice aspect person number gender mood inflection express one grammatical category prefix suffix infix another internal modification vowel change stem root part word add inflectional change derive affix ed ize de mi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are using textblob lemmatizer for cleaned text.\n",
        "textblob - textblob is used for process text based data.its provide the pattern search tagging, sentimental analysis etc.\n",
        "\n",
        "final_text - final text is the final text after stopwords and punctuation. now we are using this this final text for using textblob.\n",
        "\n",
        "lem[] = lem is the array of lemmitization.\n",
        "\n",
        "now, we are using for loop for lemmatize words based on conditions and then append words and print them.\n",
        "\n",
        "lemmatize() - this function is used for single words."
      ],
      "metadata": {
        "id": "u6BR90iIcaG2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v93jILmOT9mG"
      },
      "source": [
        "#### **Question 4. Can you think of any any differences while performing lemmatization and stemming? If yes, write them in your own words. Also write down your observations on performing lemmatization and stemming on text before and after cleaning (removing punctuation and stopwords) (10 points)**\n",
        "\n",
        "**IMPORTANT NOTE: Your observations should not be based on just Q2 and Q3. Your observations should characterize both the method as a whole. Bring out the differences also if there are any**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS-ZjxoOURAJ"
      },
      "source": [
        "**Answer for Q4.:**\n",
        "\n",
        "**Lemmatization:**\n",
        "Lemmatization means its use while doing text process of vocabulary and morphological words. its remove the human inflectional words and convert into base form.\n",
        "example of packages : wordnet lemmatization,spacy lemmatization, textblob etc..\n",
        "\n",
        "**Stemming:**\n",
        "stemming is the process for stemming word to the root of the words states as 'lemmas'.\n",
        "example of packages : 'crying' is the word and its end with 'ing' stem remove ing and its convert to the base form 'cry'.\n",
        "\n",
        "**Different between stemming and lemmatization:**\n",
        "Focusing on the previous examples, we can easily identify that stemming is stem the word while lemmatize use for context of the words. The main different is how they work for reducing words ending.\n",
        "\n",
        "In conclusion, we can see the results as stemming only remove the end of the words and doesn't check whether that type of words available or not in english dictionary such as \"cakes\" so stemming remove \"es\" from the end and print as \"cak\" there is no word as cak in english. However, talking about the lemmatization lemmatization remove the words ending but its also check with the english dictionary whether that type of word is available or not and then according to final result its print the words such as \"companies\" so its remove \"ies\" from the end and then check the dictionary for similar kind of words and then print the converted word \"company\".\n",
        "\n",
        "**Result of stemming and lemmatization after and before cleand text:**\n",
        "takling about performing stemming and lemmatize before cleaned text.focusing on the first result lemmatize before clean text its print whole text same only change some words into the base word such as \"categories\" convert to \"category\" and \"affexes\" convert to \"affex\". In contrast, on the another hand while taking about stemming, stemming remove ending of the words and doesn't check whether this type of english word is exist or not and print the result focusing of stemming results \"different\" word remove ending \"erent\" and print\"diff\" and \"grammatical\" word remove \"atical\" and print \"gramm\" which is change the meaning of word but stemming print that word.when we use stemming and lemmatization before cleaned text result prints with all the unnecessary words which is not going to use for search pattern match after stopwords and punctuation remove the results are more specific and more clear.its directly search for the content using each and every tokens. basically, after cleanded text the result must be accurate as well as matches with the pattern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJNe9XfmOcqi"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lixVq2oOP1VP"
      },
      "source": [
        "## **(Tutorial) Sentence Segmentation using Spacy**\n",
        "\n",
        "Following is a dummy paragraph of text to demonstrate how to use SpaCy to segment text into sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuFV_nccQq6u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "6a4a3a28-e188-4708-c833-0c94209be9dc"
      },
      "source": [
        "dummy_text3 = \"\"\"Here is the First Paragraph and this is the First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the first paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
        "Now, it is the Second Paragraph and its First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the second paragraph. This paragraph is ending now with a Fifth Sentence.\n",
        "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the third paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
        "4th paragraph just has one sentence in it.\n",
        "\"\"\"\n",
        "\n",
        "print(dummy_text3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the First Paragraph and this is the First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the first paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
            "Now, it is the Second Paragraph and its First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the second paragraph. This paragraph is ending now with a Fifth Sentence.\n",
            "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the third paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
            "4th paragraph just has one sentence in it.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we just print the dummy text."
      ],
      "metadata": {
        "id": "WcPHSdVuMMDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code for sentence segmentation using Spacy**"
      ],
      "metadata": {
        "id": "1BusKIBxKydQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtuBXdrAQC94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "d71f17d0-d57a-4a60-ccb3-856ac3b470a8"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# performing sentence splitting...\n",
        "doc = nlp(dummy_text3)\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the First Paragraph and this is the First Sentence.\n",
            "Here is the Second Sentence.\n",
            "Now is the Third Sentence.\n",
            "This is the Fourth Sentence of the first paragaraph.\n",
            "This paragraph is ending now with a Fifth Sentence.\n",
            "\n",
            "Now, it is the Second Paragraph and its First Sentence.\n",
            "Here is the Second Sentence.\n",
            "Now is the Third Sentence.\n",
            "This is the Fourth Sentence of the second paragraph.\n",
            "This paragraph is ending now with a Fifth Sentence.\n",
            "\n",
            "Finally, this is the Third Paragraph and is the First Sentence of this paragraph.\n",
            "Here is the Second Sentence.\n",
            "Now is the Third Sentence.\n",
            "This is the Fourth Sentence of the third paragaraph.\n",
            "This paragraph is ending now with a Fifth Sentence.\n",
            "\n",
            "4th paragraph just has one sentence in it.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are using spacy to sentence segmentation. spacy is basically used for build system for data extraction for natural language processing.\n",
        "\n",
        "import spacy - here, we are importing spacy library which is used for data extraction and sentence segmentation.\n",
        "\n",
        "spacy.load() - spacy load is used for loading english dictionaries which contains vocabulary, formulas and entities. Its useful for sentence segmentation.\n",
        "\n",
        "doc = variable which is used for nlp() method to include dummy text.nlp is natural language processing.\n",
        "\n",
        "after that we are using for loop for printing sentences in and print white spaces after ending of the each sentences.\n",
        "\n",
        "Here, we can see that each sentence is sepreat with the periods(\".\") and after that each sentence its print white spaces."
      ],
      "metadata": {
        "id": "YQHF_q3qMUp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code for sentence segmentation using NLTK library**"
      ],
      "metadata": {
        "id": "3cbtvu9uFyIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Kq3wLQfUFihv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d2c394a2-0eab-4957-d80b-c752a4222a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we import nltk library for data processing.\n",
        "and download nltk download library for punktuation."
      ],
      "metadata": {
        "id": "tIVIk9SCW-FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"This is a very bad situation. Also I am looking good\"\n",
        "sentences=nltk.sent_tokenize(text)\n",
        "for sentence in sentences:\n",
        "  print(sentence)\n",
        "  print()"
      ],
      "metadata": {
        "id": "5VTLSZMdFFQl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "3792470e-741c-459e-bd8f-f197080cf923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a very bad situation.\n",
            "\n",
            "Also I am looking good\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk is the natural language toolkit and its used for sentence tokenize."
      ],
      "metadata": {
        "id": "S62KCy4XX4US"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtQeRGq9Q_mz"
      },
      "source": [
        "\n",
        "### **Task 3. Segmenting Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg4DnVjvSoTi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "90b4da6c-dc5e-44c5-afac-17fb82974afc"
      },
      "source": [
        "seg_text=\"\"\"There are numerous different versions of portions from Lorem Ipsum that are accessible, but the most have been changed in some way,\n",
        "usually by adding humor or randomizing phrases that don't look the slightest bit plausible. If you plan to quote from Lorem Ipsum, make sure there\n",
        "is nothing unpleasant tucked away in the center of the text. This is the first real generator on the Internet because all other Lorem Ipsum producers\n",
        "tend to repeat specified pieces as many as needed. It creates Lorem Ipsum that appears plausible by using a vocabulary of more than 200 Latin terms\n",
        "and a few sample sentence constructions. As a result, the created Lorem Ipsum is never riddled with clichés, humor, or other uncharacteristic language.\"\"\"\n",
        "seg_texts = seg_text.replace(\".\",\"\\\\\")\n",
        "\n",
        "print(seg_texts)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are numerous different versions of portions from Lorem Ipsum that are accessible, but the most have been changed in some way,\n",
            "usually by adding humor or randomizing phrases that don't look the slightest bit plausible\\ If you plan to quote from Lorem Ipsum, make sure there\n",
            "is nothing unpleasant tucked away in the center of the text\\ This is the first real generator on the Internet because all other Lorem Ipsum producers\n",
            "tend to repeat specified pieces as many as needed\\ It creates Lorem Ipsum that appears plausible by using a vocabulary of more than 200 Latin terms\n",
            "and a few sample sentence constructions\\ As a result, the created Lorem Ipsum is never riddled with clichés, humor, or other uncharacteristic language\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVHl6V3eVudZ"
      },
      "source": [
        "#### **Question 5a. Write a python code for sentence segmentation using a backslash as the boundry for each sentence for the text above. Write your observations in your own words for the genereated output.(15 points)**\n",
        "\n",
        "**Note:** You do not need to remove any stopwords, punctuation or apply any kind of other preprocessing techniques. Only perform what's asked to minimize your effort needed to answer this question.\n",
        "\n",
        "**Hint**: Use print( ) to help you understand how the sentences are being split when analyzing your output to note down your observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZBRbBDuWogQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "5cb76a61-4655-4134-9703-96642666a197"
      },
      "source": [
        "# write your code below this comment\n",
        "#here is the text with the backslash for sentence segmentation.\n",
        "seg_texts = seg_text.replace(\".\",\"\\\\\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences=nltk.sent_tokenize(seg_texts)\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(sentence)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are numerous different versions of portions from Lorem Ipsum that are accessible, but the most have been changed in some way,\n",
            "usually by adding humor or randomizing phrases that don't look the slightest bit plausible\\ If you plan to quote from Lorem Ipsum, make sure there\n",
            "is nothing unpleasant tucked away in the center of the text\\ This is the first real generator on the Internet because all other Lorem Ipsum producers\n",
            "tend to repeat specified pieces as many as needed\\ It creates Lorem Ipsum that appears plausible by using a vocabulary of more than 200 Latin terms\n",
            "and a few sample sentence constructions\\ As a result, the created Lorem Ipsum is never riddled with clichés, humor, or other uncharacteristic language\\\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this is with the regular text ending sentences with the periods(\".\")\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences=nltk.sent_tokenize(seg_text)\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(sentence)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "leP0Yps7XjWm",
        "outputId": "c906a221-f5fc-4435-efe7-240185370392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are numerous different versions of portions from Lorem Ipsum that are accessible, but the most have been changed in some way,\n",
            "usually by adding humor or randomizing phrases that don't look the slightest bit plausible.\n",
            "\n",
            "If you plan to quote from Lorem Ipsum, make sure there\n",
            "is nothing unpleasant tucked away in the center of the text.\n",
            "\n",
            "This is the first real generator on the Internet because all other Lorem Ipsum producers\n",
            "tend to repeat specified pieces as many as needed.\n",
            "\n",
            "It creates Lorem Ipsum that appears plausible by using a vocabulary of more than 200 Latin terms\n",
            "and a few sample sentence constructions.\n",
            "\n",
            "As a result, the created Lorem Ipsum is never riddled with clichés, humor, or other uncharacteristic language.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, as per the result shown in above example if we change end of the sentence peiods(.) with backslash then results are shown that sentence segmentation is not working properly.its continuing as a whole paragraph without understanding that its end of the sentence so we need to split the sentence.i replaced each end of the sentence period with the backslash and when i try to split the sentences then using nltk its not working properly. so, In conclusion we can say that nltk only idenfy that period(\".\") is the only end of sentence after long bunch of words and backslash(\" \\ \") is not consider as a end of the sentence.\n",
        "\n",
        "first i used string.replace method and pass the argument to replace \"\\\" with the \".\"\n",
        "\n",
        "then import the nltk library and download the punctuation library.\n",
        "\n",
        "now using nltk- natural language toolkit do the sentence tokenize using sent_tokenize() method.\n",
        "\n",
        "after that we are using for loop for printing that processed sentences one by one with end od sentence punctuation using backslash.\n",
        "\n",
        "Overall, focusing on the final results its prove that nltk is take period(\".\") as the end of the sentence by default but if we use backslash as the end of sentence then its couldn't identify as a sentence end and its continue the next sentence without parting.\n",
        "\n",
        "\n",
        "Focusing on the second example  its shows that when use period(\".\") as the end of the sentence then its easily identify the end of the sentence and split from there."
      ],
      "metadata": {
        "id": "hlwZr0xVQnzd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiEaJ3RuSmeU"
      },
      "source": [
        "#### **Question 5b. Perform sentence segmentation using spacy on the the text used in question 5a(seg_text) (5 points)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "#load spacy for core english library\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(seg_texts)\n",
        "#to print sentences\n",
        "for sent in doc.sents:\n",
        "  print(sent)\n",
        ""
      ],
      "metadata": {
        "id": "abjTg_e49x8u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "51a4dd68-3981-479e-f4f8-e68133458188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are numerous different versions of portions from Lorem Ipsum that are accessible, but the most have been changed in some way,\n",
            "usually by adding humor or randomizing phrases that don't look the slightest bit plausible\\\n",
            "If you plan to quote from Lorem Ipsum, make sure there\n",
            "is nothing unpleasant tucked away in the center of the text\\\n",
            "This is the first real generator on the Internet because all other Lorem Ipsum producers\n",
            "tend to repeat specified pieces as many as needed\\ It creates Lorem Ipsum that appears plausible by using a vocabulary of more than 200 Latin terms\n",
            "and a few sample sentence constructions\\\n",
            "As a result, the created Lorem Ipsum is never riddled with clichés, humor, or other uncharacteristic language\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "#load spacy for core english library\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(seg_text)\n",
        "#to print sentences\n",
        "for sent in doc.sents:\n",
        "  print(sent)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "390zE0MEYCaq",
        "outputId": "6ab9abf6-f358-48fb-ba7d-44d9e5c544f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are numerous different versions of portions from Lorem Ipsum that are accessible, but the most have been changed in some way,\n",
            "usually by adding humor or randomizing phrases that don't look the slightest bit plausible.\n",
            "If you plan to quote from Lorem Ipsum, make sure there\n",
            "is nothing unpleasant tucked away in the center of the text.\n",
            "This is the first real generator on the Internet because all other Lorem Ipsum producers\n",
            "tend to repeat specified pieces as many as needed.\n",
            "It creates Lorem Ipsum that appears plausible by using a vocabulary of more than 200 Latin terms\n",
            "and a few sample sentence constructions.\n",
            "As a result, the created Lorem Ipsum is never riddled with clichés, humor, or other uncharacteristic language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we try to use spacy for sentence segmentation.\n",
        "spacy is the open source library for build system for extraction data.\n",
        "import spacy - we are importing spacy library for sentence segmentation.\n",
        "spacy.load() -  spacy load is used for loading english vocabularies and entities.\n",
        "doc -  doc is the variable and stored sentence after using nlp(natural language processing).\n",
        "for loop - here, we are using for loop for printing sentences while using natural language processed data.\n",
        "\n",
        "sent - sent is the processed data for natural language processed sentences and print them.\n",
        "\n",
        "To summerize, focusing on the final result its shows that spacy include backslash (\"\\\") special character as a end of the sentence after long bunch of words and its split sentences in to different sentence tokens. In contrast, in the above example we use the nltk for sentence segmentation and its not work properly.\n",
        "\n",
        "On the other hand try to use period(\".\") as the end of the sentence and focusing on the results spacy include after the long bunch of words and periods occure then its consider as a end of the sentence aslo in short spacy include all the punctuations as the end of the sentence."
      ],
      "metadata": {
        "id": "QSuyxLz8YtjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 5c. Perform sentence segmentation using NLTK on the the text used in question 5a(seg_text) (5 points)**\n",
        "\n",
        "**Hint**: For implementing NLTK's sentence segmentation, you can refer to the code block above."
      ],
      "metadata": {
        "id": "tduLcCup9yia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code below this comment\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "sentences = nltk.sent_tokenize(seg_texts)  #whole paragraph break into sentence.\n",
        "for sentence in sentences:\n",
        "\tprint(sentence)\n",
        "\tprint()"
      ],
      "metadata": {
        "id": "J2SNmvLr90cM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "268a2f69-19b4-4cab-dd56-fb2e21ad7663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are numerous different versions of portions from Lorem Ipsum that are accessible, but the most have been changed in some way,\n",
            "usually by adding humor or randomizing phrases that don't look the slightest bit plausible\\ If you plan to quote from Lorem Ipsum, make sure there\n",
            "is nothing unpleasant tucked away in the center of the text\\ This is the first real generator on the Internet because all other Lorem Ipsum producers\n",
            "tend to repeat specified pieces as many as needed\\ It creates Lorem Ipsum that appears plausible by using a vocabulary of more than 200 Latin terms\n",
            "and a few sample sentence constructions\\ As a result, the created Lorem Ipsum is never riddled with clichés, humor, or other uncharacteristic language\\\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code below this comment\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "sentences = nltk.sent_tokenize(seg_text)  #whole paragraph break into sentence.\n",
        "for sentence in sentences:\n",
        "\tprint(sentence)\n",
        "\tprint()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "yx6EkEjeaWD0",
        "outputId": "fdff4036-ed64-42b1-aad8-e8648aefb98b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are numerous different versions of portions from Lorem Ipsum that are accessible, but the most have been changed in some way,\n",
            "usually by adding humor or randomizing phrases that don't look the slightest bit plausible.\n",
            "\n",
            "If you plan to quote from Lorem Ipsum, make sure there\n",
            "is nothing unpleasant tucked away in the center of the text.\n",
            "\n",
            "This is the first real generator on the Internet because all other Lorem Ipsum producers\n",
            "tend to repeat specified pieces as many as needed.\n",
            "\n",
            "It creates Lorem Ipsum that appears plausible by using a vocabulary of more than 200 Latin terms\n",
            "and a few sample sentence constructions.\n",
            "\n",
            "As a result, the created Lorem Ipsum is never riddled with clichés, humor, or other uncharacteristic language.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are using nltk- natural language toolkit for processed sentences from the paragraph.\n",
        "\n",
        "import nltk - we are importing nltk library for doing sentence segmentation.\n",
        "\n",
        "nltk.sent_tokenize() -  this is used for nltk.sent_tokenize() method for sentence tokenize.\n",
        "\n",
        "we are using for loop for printing sentence segmentation final sentences after processed natural language processing.\n",
        "\n",
        "To conclude, its shows in the result that nltk is not spliting sentences while using backslash as the end of sentences its not identify that backslash is the end of sentence and its continue print the results whitout splitting the sentences.On the other hand, while use period(\".\") as the end of the sentence its clearly identify as a end of the sentence and its split the sentences."
      ],
      "metadata": {
        "id": "xXFtOQtnczeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 5d. Analyze the generated output from question 5b and 5c. Provide your observations in your own words. (5 points)**"
      ],
      "metadata": {
        "id": "6QHpGz5O93R9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER 5d: Here, we are using nltk and spacy for sentence segmentation.\n",
        "\n",
        "**Sentence segmentation:**\n",
        "The process of where is staring and ending of the sentence and devide that paragraph using NLP- natural language processing in sentences.\n",
        "\n",
        "Here, we can see the main difference is how punctuation marks changes the whole results.\n",
        "\n",
        "**nltk sentence segmentation :**\n",
        "\n",
        "Here, we use nltk.sent_tokenize() method for sentence tokenize using nltk.\n",
        "\n",
        "nltk sentence segmentation is used for split each sentences with periods(\".\") at the end of the sentence. nltk use sent_tokenize() method for sentence segmentation.\n",
        "nltk consider \".\", \"?\", \"!\" as the end of the sentences or we can say sentence boundries. so, backslash(\"\\\" ) is not consider as a boundry of sentences.\n",
        "\n",
        "Here, i used two different punctuation for end of the sentences (\".\") and(\"\\\") and its shows that backslash is not take as the sentence boundries but if we are using periods(\".\") then its consider as boundries and split the sentences and print them.\n",
        "\n",
        "nltk have their own restriction for rules of sentence boundries.\n",
        "\n",
        "**Spacy sentence segmentation:**\n",
        "\n",
        "spacy is the open source library its used for tokenizing words and sentences.\n",
        "\n",
        "spacy using nlp object to convert text in to doc text and split paragraph in to sentences.\n",
        "\n",
        "spacy sentence segmentation is used for split paragraph into sentences using nlp-natural language processing.\n",
        "\n",
        "We provide two different set of examples for sentence boundaries (\".\") and (\"\\\"). spacy identify both of them as  the sentence boundaries and split the paragraph in to sentences.\n",
        "\n",
        "to sum up, we can say that spacy allows customization rule based sentence boundaries and we can customize as per our needs and its provide by default punctuation also so based on whole text processing its identify that this is used as a boundaries and print the next sentence.\n",
        "\n",
        "\n",
        "In summarize, nltk tool not allow to customize the sentence boundries while, spacy does.\n"
      ],
      "metadata": {
        "id": "86s70XLv96BA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYE8A-Mk4BKe"
      },
      "source": [
        "## **(Tutorial) Subword Tokenization using HuggingFace**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4J8U2pWSc8v"
      },
      "source": [
        "### **Task 4: Subword Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS4dlR20V1eX"
      },
      "source": [
        "Well, the natural language processing is not as intelligent as we humans are, and not so intellectual to break words into sub words and try to decipher the word if it sees a word that is not in the corpus yet. This is where Subword Tokenization comes into picture.\n",
        "\n",
        "Subword tokenization is a recent strategy from machine translation that helps us solve these problems by breaking unknown words into “subword units” - strings of characters like ing or eau - that still allow the downstream model to make intelligent decisions on words it doesn't recognize.\n",
        "\n",
        "\n",
        "**Here we will implement the basic BertTokenizer using a pretrained model**\n",
        "\n",
        "BERT stands for Bidirectional Encoder Representations from Transformers. BERT is a transformers model that was self-supervisedly pretrained on a sizable corpus of English data. This indicates that it was trained exclusively on raw texts, with no humans tagging in any kind. As a result, it may employ a large amount of data that is readily available to the general public, with an automatic procedure to produce inputs and tags from such texts. The uncased model tokenizes the words which is not present in the vocabulary.\n",
        "\n",
        "**Below is the implementation of the Bert Tokenizer:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "EZ6Q1AMKi2Kg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "b20d0c99-4766-4314-d4c8-0e6df2f977b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.8/dist-packages (0.13.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenization :\n",
        "tokenization is the process of spliting paragraphs into tokens using natural language processing.Here, we try to install tokenize library for doing tokenizing using natural language processing.\n",
        "\n",
        "Transformers:\n",
        "transformers is the library which is provide lots of pretrained model of text, video and audio data.here, this is basically use for perfom some different task for text."
      ],
      "metadata": {
        "id": "QFO7peOesUrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer.tokenize(\"Because he was late again, he would be docked a day's pay\")"
      ],
      "metadata": {
        "id": "WbQGR-9XlWUa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "1dac52fb-c8f9-4c0d-d3a3-9c5a1ad8be4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['because',\n",
              " 'he',\n",
              " 'was',\n",
              " 'late',\n",
              " 'again',\n",
              " ',',\n",
              " 'he',\n",
              " 'would',\n",
              " 'be',\n",
              " 'docked',\n",
              " 'a',\n",
              " 'day',\n",
              " \"'\",\n",
              " 's',\n",
              " 'pay']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are using BertTokenizer for processing data. Bert main use is to tokenize one word as a whole word token or else devide each word into small tokens.Bert is also known as a wordpiece tokenizer.\n",
        "\n",
        "Focusing on the example above we first import berttokenizer from transformer and use this for split sentences into wordpieces.after that we load the pretrained model of berttokenizer bert-base-uncased and then at the last we provide text with the object of berttokenizer tokenizer.\n",
        "\n",
        "In summerize, after using pretrained model of berttokenize with our sentences now focusing on the result which is we got it clearly visible that its devide the whole sentence with the each word convert into tokens.in breif its visible that its also count \",\" ,\" ' \",\"'s\" as the tokens."
      ],
      "metadata": {
        "id": "Z7yPkUP5Uyzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:** Examine these two sentences below:\n",
        "\n",
        "* The job interview was very tough, but I'm glad I made it through.\n",
        "* The new battleroyal game is super amazing!\n",
        "\n",
        "Using the Bert tokenizer(created during the tutorial) encode these two sentences using . Examine the tokens from the encodings of the two sentences. Is/Are there any interesting observations when you compare the tokens between the two encodings? What do you think is causing what you observe as part of your comparison? **(20points)**\n"
      ],
      "metadata": {
        "id": "jD90aNX4kLAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the BertTokenizer that was created during the tutorial to encode the sentences\n",
        "# write your code below this comment and execute\n",
        "# type in your answer to the question asked above in the following cell (see below)\n",
        "#this is for Sentence 1\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer.tokenize(\"The job interview was very tough, but I'm glad I made it through.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "zSqRMo0dYXaZ",
        "outputId": "5b955ad9-14ec-42e7-8391-14f95e990d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'job',\n",
              " 'interview',\n",
              " 'was',\n",
              " 'very',\n",
              " 'tough',\n",
              " ',',\n",
              " 'but',\n",
              " 'i',\n",
              " \"'\",\n",
              " 'm',\n",
              " 'glad',\n",
              " 'i',\n",
              " 'made',\n",
              " 'it',\n",
              " 'through',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this is for sentence two\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer.tokenize(\"The new battleroyal game is super amazing!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "3tf5GWf4YbYN",
        "outputId": "10ef10c1-23cf-454f-b7bc-e2bd1687c2cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'new',\n",
              " 'battle',\n",
              " '##roy',\n",
              " '##al',\n",
              " 'game',\n",
              " 'is',\n",
              " 'super',\n",
              " 'amazing',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this is for sentence two\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer.tokenize(\"The new Battle Royal game is super amazing!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "rzx41jlSkb47",
        "outputId": "c690c518-7723-431d-bd5b-0f332090a68a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'new', 'battle', 'royal', 'game', 'is', 'super', 'amazing', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer to Question 6:**\n",
        "\n",
        "Let's start with the basic hugging face.\n",
        "\n",
        "**Hugging Face :**\n",
        "Hugging face usually do devide paragraph or text into tokens and check with vocabulary and add some tokens too.\n",
        "\n",
        "**Tokenizer:**\n",
        "Tokenizer is used for devide paragraph or text into sentences and sentences into words.\n",
        "\n",
        "**Transformers:**\n",
        "Transformers is used as processed data of text,video and audio with pretrained model.transformers also provide some pretrained models for text classification ,sentiment analysis etc.\n",
        "\n",
        "**Berttokenizer:**\n",
        "Berttokenizer is used for tokenize the sentences into words. bert is work two ways for tokenize, first way is its take one whole word as the one token second is its also devide each words into small tokens.\n",
        "\n",
        "Focusing on the examples above we discussed below how it works:\n",
        "\n",
        "**from transformers import BertTokenizer**\n",
        "This is we use for using berttokenizer library from transformers library.we are using this for text processing and we can say using for tokenize the sentences.\n",
        "\n",
        "**tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')**\n",
        "This is we are using for use text processing or tokenization pretrained model from the berttokenizer. name of the pretrained model is bert-base-uncased which is generally used for text processing and tokenizing the words.\n",
        "\n",
        "\n",
        "**tokenizer.tokenize(\"The job interview was very tough, but I'm glad I made it through.\")**\n",
        "**tokenizer.tokenize(\"The new battleroyal game is super amazing!\") **\n",
        "This two sentences we are using for making tokens from the sentences. that's why we use tokenizer object of berttokenize pretrained model and use here with tokenize() method to do the tokenize of that sentences.\n",
        "\n",
        "**Different between two sentence output**\n",
        "\n",
        "To recapitulate we are using the berttokenize for split sentences into words or each words split into small tokens.let's discuss the main point of different output.\n",
        "\n",
        "The main difference is focusing on the output of first sentence its devide each words into tokens its consider \",\" and \".\" also taken as tokens.its devide word \"I'm\" as a three different tokens.\"I\",\"'\",\"m\" etc.at the last period \".\" of the sentence also taken as a whole token.However, on the other hand, in the second example its devide sentence into each word tokens but main thing is focusing here is its devide the word \"battleroyal\"words into three different words follow as \"battle\",\"##roy\",\"##al\" etc.\n",
        "\n",
        "\n",
        "The main thing is first sentence \"The job interview was very tough, but I'm glad I made it through.\". focusing on the results its shows its split sentence into words token but its take punctuation marks also as a token and add sentence boundaries periods\".\" is also taken as a tokens.\n",
        "\n",
        "the second sentence is \"The new battleroyal game is super amazing!\" here its devide sentence into words tokens but because the word \"battleroyal\" is taken as a lowercase so model doesn't identify that battleroyal is the game name its noun and taken as a three different token are as follow \"bottle\",\"##roy\",\"##al\".at the last \"!\" mark is also use as a token.so, if the word \"bottleroyal\" write it down in the first capital letter \"Battle Royal\" then results of the tokens will be different."
      ],
      "metadata": {
        "id": "5dLOpj9amC9Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K9BsX3KcC6y"
      },
      "source": [
        "## **References**\n",
        "* https://spacy.io/usage/spacy-101\n",
        "* https://spacy.io/models/en\n",
        "* https://www.geeksforgeeks.org/python-nltk-tokenize-wordpuncttokenizer/\n",
        "* https://neptune.ai/blog/tokenization-in-nlp\n",
        "* https://www.datacamp.com/tutorial/stemming-lemmatization-python\n",
        "* https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/\n",
        "*https://colab.research.google.com/drive/10gwzRY55JqzgeEQOX6nwFs6bQ84-mB9f?usp=sharing#scrollTo=DP1xuStV0fDl\n",
        "*https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c\n",
        "*https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/\n",
        "*https://huggingface.co/transformers/v3.0.2/tokenizer_summary.html"
      ]
    }
  ]
}